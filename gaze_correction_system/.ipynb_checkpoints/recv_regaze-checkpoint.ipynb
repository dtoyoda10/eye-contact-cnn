{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load package and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import sys\n",
    "import dlib\n",
    "import time\n",
    "import socket\n",
    "import struct\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from win32api import GetSystemMetrics\n",
    "import win32gui\n",
    "\n",
    "from threading import Thread, Lock\n",
    "from config import get_config\n",
    "import pickle\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(P_IDP=5, agl_dim=2, channel=3, ef_dim=12, encoded_agl_dim=16, f=520, height=48, mod='flx', record_time=False, recver_port=5005, sender_port=5005, tar_ip='localhost', uid='local', weight_set='weights_20180413', width=64)\n"
     ]
    }
   ],
   "source": [
    "conf,_ = get_config()\n",
    "if conf.mod == 'flx_0413':\n",
    "    import flx_0413 as model\n",
    "elif conf.mod == 'deepwarp':\n",
    "    import deepwarp as model\n",
    "else:\n",
    "    sys.exit(\"Wrong Model selection: flx or deepwarp\")\n",
    "\n",
    "# system parameters\n",
    "model_dir = '../'+conf.weight_set+'/warping_model/ckpt/'+conf.mod+'/'+ str(conf.ef_dim) + '/'\n",
    "size_video = [640,480]\n",
    "coor_remote_head_center = [320,240]\n",
    "fps = 0\n",
    "P_IDP = 5\n",
    "depth = -50\n",
    "# for monitoring\n",
    "fig_alpha = [0,0]\n",
    "fig_R_w=[320,240]\n",
    "fig_eye_pos = [0,0,0]\n",
    "\n",
    "# environment parameter\n",
    "Rs = (GetSystemMetrics(0),GetSystemMetrics(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_window = [659,528]\n",
    "win_pos = [int(Rs[0]/2)-int(size_window[0]/2),int(Rs[1]/2)-int(size_window[1]/2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load CNN models to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model of [L] eye to GPU\n",
      "INFO:tensorflow:Restoring parameters from ../weights_20180413/warping_model/ckpt/flx/12/L/mix_48x64_L5-15308\n",
      "Loading model of [R] eye to GPU\n",
      "INFO:tensorflow:Restoring parameters from ../weights_20180413/warping_model/ckpt/flx/12/R/mix_48x64_R2-10680\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading model of [L] eye to GPU\")\n",
    "with tf.Graph().as_default() as g:\n",
    "    # define placeholder for inputs to network\n",
    "    with tf.name_scope('inputs'):\n",
    "        LE_input_img = tf.placeholder(tf.float32, [None, conf.height, conf.width, conf.channel], name=\"input_img\") # [None, 41, 51, 3]\n",
    "        LE_input_fp = tf.placeholder(tf.float32, [None, conf.height, conf.width,conf.ef_dim], name=\"input_fp\") # [None, 41, 51, 14]\n",
    "        LE_input_ang = tf.placeholder(tf.float32, [None, conf.agl_dim], name=\"input_ang\") ## [None, 41, 51, 2]\n",
    "        LE_phase_train = tf.placeholder(tf.bool, name='phase_train') # a bool for batch_normalization\n",
    "\n",
    "    # inference model.\n",
    "    LE_img_pred, _, _ = model.inference(LE_input_img, LE_input_fp, LE_input_ang,  LE_phase_train, conf)\n",
    "\n",
    "    # split modle here\n",
    "    L_sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,log_device_placement=False), graph = g)\n",
    "    # load model\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    ckpt = tf.train.get_checkpoint_state(model_dir+'L/')\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        # Restores from checkpoint\n",
    "        saver.restore(L_sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        print('No checkpoint file found')\n",
    "        \n",
    "print(\"Loading model of [R] eye to GPU\")\n",
    "with tf.Graph().as_default() as g2:\n",
    "    # define placeholder for inputs to network\n",
    "    with tf.name_scope('inputs'):\n",
    "        RE_input_img = tf.placeholder(tf.float32, [None, conf.height, conf.width, conf.channel], name=\"input_img\") # [None, 41, 51, 3]\n",
    "        RE_input_fp = tf.placeholder(tf.float32, [None, conf.height, conf.width,conf.ef_dim], name=\"input_fp\") # [None, 41, 51, 14]\n",
    "        RE_input_ang = tf.placeholder(tf.float32, [None, conf.agl_dim], name=\"input_ang\") ## [None, 2]\n",
    "        RE_phase_train = tf.placeholder(tf.bool, name='phase_train') # a bool for batch_normalization\n",
    "\n",
    "    # inference model.\n",
    "    RE_img_pred, _, _ = model.inference(RE_input_img, RE_input_fp, RE_input_ang, RE_phase_train, conf)\n",
    "\n",
    "    # split modle here\n",
    "    R_sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True,log_device_placement=False), graph = g2)\n",
    "    # load model\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    ckpt = tf.train.get_checkpoint_state(model_dir+'R/')\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        # Restores from checkpoint\n",
    "        saver.restore(R_sess, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        print('No checkpoint file found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class video_receiver:\n",
    "#     def __init__(self, port):\n",
    "#         self.close = False\n",
    "#         self.port = port\n",
    "#         self.video_recv = socket.socket(socket.AF_INET,socket.SOCK_STREAM)\n",
    "#         print('Socket created')\n",
    "#         self.detector = dlib.get_frontal_face_detector()\n",
    "#         self.predictor = dlib.shape_predictor(\"./lm_feat/shape_predictor_68_face_landmarks.dat\") \n",
    "#         self.face_detect_size = [320,240]\n",
    "#         self.coor_remote_head_center = [320,240]\n",
    "#         self.x_ratio = size_video[0]/self.face_detect_size[0]\n",
    "#         self.y_ratio = size_video[1]/self.face_detect_size[1]\n",
    "        \n",
    "#         self.video_recv_thread = Thread(target=self.start_recv, args=(self.port,))\n",
    "#         self.video_recv_thread.start()\n",
    "        \n",
    "#     def face_detection(self,frame):\n",
    "#         # face detection\n",
    "#         gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "#         face_detect_gray = cv2.resize(gray,(self.face_detect_size[0],self.face_detect_size[1]))\n",
    "#         detections = self.detector(face_detect_gray, 0)\n",
    "#         for k,bx in enumerate(detections):\n",
    "#             #(left,right,top,bottom)\n",
    "#             self.coor_remote_head_center = [int((bx.left()+bx.right())*self.x_ratio/2),\n",
    "#                                        int((bx.top()+bx.bottom())*self.y_ratio/2)]\n",
    "#             break\n",
    "# #         print(recv_time_fd)\n",
    "# #         print(self.coor_remote_head_center)\n",
    "#         remote_head_Center = self.coor_remote_head_center\n",
    "        \n",
    "#     def start_recv(self, port):\n",
    "#         global remote_head_Center\n",
    "#         self.video_recv.bind(('',port))\n",
    "#         self.video_recv.listen(10)\n",
    "#         print('Socket now listening')\n",
    "#         conn,addr=self.video_recv.accept()\n",
    "\n",
    "#         data = b\"\"\n",
    "#         payload_size = struct.calcsize(\">L\")\n",
    "#         print(\"payload_size: {}\".format(payload_size))\n",
    "        \n",
    "#         while True:\n",
    "#             while len(data) < payload_size:\n",
    "#                 data += conn.recv(90456)\n",
    "\n",
    "#             packed_msg_size = data[:payload_size]\n",
    "#             data = data[payload_size:]\n",
    "#             msg_size = struct.unpack(\">L\", packed_msg_size)[0]\n",
    "#             while len(data) < msg_size:\n",
    "#                 data += conn.recv(90456)\n",
    "\n",
    "#             frame_data = data[:msg_size]\n",
    "#             data = data[msg_size:]\n",
    "\n",
    "#             frame = pickle.loads(frame_data, fix_imports=True, encoding=\"bytes\")           \n",
    "            \n",
    "#             frame = cv2.imdecode(frame, cv2.IMREAD_COLOR)\n",
    "            \n",
    "# #             self.video_recv_hd_thread = Thread(target=self.face_detection, args=(frame,))\n",
    "# #             self.video_recv_hd_thread.start()\n",
    "\n",
    "#             cv2.imshow('Remote',frame)\n",
    "#             k = cv2.waitKey(1)\n",
    "#             if (k == ord('o') or self.close):                \n",
    "#                 self.close = True\n",
    "#                 break\n",
    "#             else:\n",
    "#                 pass\n",
    "#         cv2.destroyWindow('Remote')\n",
    "            \n",
    "#     def stop(self):\n",
    "#         self.close = True\n",
    "#         if self.video_recv_thread.is_alive():\n",
    "#             self.video_recv_thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # _ = video_receiver(port = conf.recver_port)\n",
    "\n",
    "# vs_thread = Thread(target=video_receiver, args=(conf.recver_port,))\n",
    "# vs_thread.start()\n",
    "# # time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## socket\n",
    "# client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "# client_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
    "# client_socket.connect((conf.tar_ip, conf.sender_port))\n",
    "# encode_param=[int(cv2.IMWRITE_JPEG_QUALITY),90]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flx-gaze "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gaze_redirection_system:\n",
    "    def __init__(self):\n",
    "        #Landmark identifier. Set the filename to whatever you named the downloaded file\n",
    "        self.detector = dlib.get_frontal_face_detector()\n",
    "        self.predictor = dlib.shape_predictor(\"./lm_feat/shape_predictor_68_face_landmarks.dat\") \n",
    "        self.size_df = [320,240]\n",
    "        self.size_I = [48,64]\n",
    "        # initial value\n",
    "        self.Rw = [0,0]\n",
    "        self.Pe_z = -50\n",
    "        ####換電腦記得重新量下面參數！\n",
    "        self.f = conf.f\n",
    "        self.Ps = (35,19.5)\n",
    "        self.Pc = (0,-10,0)\n",
    "        self.Pe = [self.Pc[0],self.Pc[1],self.Pe_z] # H,V,D\n",
    "        \n",
    "    def monitor_para(self,frame):\n",
    "        global fig_eye_pos\n",
    "        global fig_alpha\n",
    "        global fig_R_w\n",
    "        cv2.rectangle(frame,\n",
    "                  (size_video[0]-150,0),(size_video[0],55),\n",
    "                  (255,255,255),-1\n",
    "                 )\n",
    "        cv2.putText(frame,\n",
    "                    'Eye:['+str(int(fig_eye_pos[0])) +','+str(int(fig_eye_pos[1]))+','+str(int(fig_eye_pos[2]))+']',\n",
    "                    (size_video[0]-140,15), cv2.FONT_HERSHEY_SIMPLEX, 0.4,(0,0,255),1,cv2.LINE_AA)\n",
    "        cv2.putText(frame,\n",
    "                    'alpha:[V='+str(int(fig_alpha[0])) + ',H='+ str(int(fig_alpha[1]))+']',\n",
    "                    (size_video[0]-140,30),cv2.FONT_HERSHEY_SIMPLEX,0.4,(0,0,255),1,cv2.LINE_AA)\n",
    "        cv2.putText(frame,\n",
    "                    'R_w:['+str(int(fig_R_w[0])) + ','+ str(int(fig_R_w[1]))+']',\n",
    "                    (size_video[0]-140,45),cv2.FONT_HERSHEY_SIMPLEX,0.4,(0,0,255),1,cv2.LINE_AA)\n",
    "        return frame\n",
    "        \n",
    "    def get_inputs(self, frame, shape, pos = \"L\", size_I = [48,64]):\n",
    "        if(pos == \"R\"):\n",
    "            lc = 36\n",
    "            rc = 39\n",
    "            FP_seq = [36,37,38,39,40,41]\n",
    "        elif(pos == \"L\"):\n",
    "            lc = 42\n",
    "            rc = 45\n",
    "            FP_seq = [45,44,43,42,47,46]\n",
    "        else:\n",
    "            print(\"Error: Wrong Eye\")\n",
    "\n",
    "        eye_cx = (shape.part(rc).x+shape.part(lc).x)*0.5\n",
    "        eye_cy = (shape.part(rc).y+shape.part(lc).y)*0.5\n",
    "        eye_center = [eye_cx, eye_cy]\n",
    "        eye_len = np.absolute(shape.part(rc).x - shape.part(lc).x)\n",
    "        bx_d5w = eye_len*3/4\n",
    "        bx_h = 1.5*bx_d5w\n",
    "        sft_up = bx_h*7/12\n",
    "        sft_low = bx_h*5/12\n",
    "        img_eye = frame[int(eye_cy-sft_up):int(eye_cy+sft_low),int(eye_cx-bx_d5w):int(eye_cx+bx_d5w)]\n",
    "        ori_size = [img_eye.shape[0],img_eye.shape[1]]\n",
    "        LT_coor = [int(eye_cy-sft_up), int(eye_cx-bx_d5w)] # (y,x)    \n",
    "        img_eye = cv2.resize(img_eye, (size_I[1],size_I[0]))\n",
    "        # create anchor maps\n",
    "        ach_map = []\n",
    "        for i,d in enumerate(FP_seq):\n",
    "            resize_x = int((shape.part(d).x-LT_coor[1])*size_I[1]/ori_size[1])\n",
    "            resize_y = int((shape.part(d).y-LT_coor[0])*size_I[0]/ori_size[0])\n",
    "            # y\n",
    "            ach_map_y = np.expand_dims(np.expand_dims(np.arange(0, size_I[0]) - resize_y, axis=1), axis=2)\n",
    "            ach_map_y = np.tile(ach_map_y, [1,size_I[1],1])\n",
    "            # x\n",
    "            ach_map_x = np.expand_dims(np.expand_dims(np.arange(0, size_I[1]) - resize_x, axis=0), axis=2)\n",
    "            ach_map_x = np.tile(ach_map_x, [size_I[0],1,1])\n",
    "            if (i ==0):\n",
    "                ach_map = np.concatenate((ach_map_x, ach_map_y), axis=2)\n",
    "            else:\n",
    "                ach_map = np.concatenate((ach_map, ach_map_x, ach_map_y), axis=2)\n",
    "\n",
    "        return img_eye/255, ach_map, eye_center, ori_size, LT_coor\n",
    "       \n",
    "    def shifting_angles_estimator(self, R_le, R_re):\n",
    "        global fig_alpha\n",
    "        global fig_R_w\n",
    "        global fig_eye_pos\n",
    "        \n",
    "        # get P_w\n",
    "        try:\n",
    "            tar_win = win32gui.FindWindow(None, \"Remote\")\n",
    "            Rw_lt = win32gui.GetWindowRect(tar_win)\n",
    "            \n",
    "        except:\n",
    "            Rw_lt = [int(Rs[0])-int(size_window[0]/2),int(Rs[1])-int(size_window[1]/2)]\n",
    "            print(\"Missing the window\")\n",
    "        R_w = (Rw_lt[0]+coor_remote_head_center[0], Rw_lt[1]+coor_remote_head_center[1])\n",
    "        fig_R_w = R_w\n",
    "        Pw = (self.Ps[0]*(R_w[0]-Rs[0]/2)/Rs[0], self.Ps[1]*(R_w[1]-Rs[1]/2)/Rs[1], 0)\n",
    "        \n",
    "        # get Pe\n",
    "        self.Pe[2] = -(self.f*P_IDP)/np.sqrt((R_le[0]-R_re[0])**2 + (R_le[1]-R_re[1])**2)\n",
    "        # x-axis needs flip\n",
    "        self.Pe[0] = -np.abs(self.Pe[2])*(R_le[0]+R_re[0]-size_video[0])/(2*self.f) + self.Pc[0]\n",
    "        self.Pe[1] = np.abs(self.Pe[2])*(R_le[1]+R_re[1]-size_video[1])/(2*self.f) + self.Pc[1]\n",
    "        fig_eye_pos = self.Pe\n",
    "\n",
    "        # calcualte alpha\n",
    "        a_w2z_x = math.degrees(math.atan( (Pw[0]-self.Pe[0])/(Pw[2]-self.Pe[2])))\n",
    "        a_w2z_y = math.degrees(math.atan( (Pw[1]-self.Pe[1])/(Pw[2]-self.Pe[2])))    \n",
    "        \n",
    "        a_z2c_x = math.degrees(math.atan( (self.Pe[0]-self.Pc[0])/(self.Pc[2]-self.Pe[2])))\n",
    "        a_z2c_y = math.degrees(math.atan( (self.Pe[1]-self.Pc[1])/(self.Pc[2]-self.Pe[2])))\n",
    "#         print('a_x=' , a_w2z_x,'+',a_z2c_x)\n",
    "#         print('a_y=' , a_w2z_y,'+',a_z2c_y)\n",
    "        alpha = [int(a_w2z_y + a_z2c_y),int(a_w2z_x + a_z2c_y)]\n",
    "       \n",
    "        fig_alpha = alpha\n",
    "            \n",
    "        return alpha\n",
    "    \n",
    "    def flx_gaze(self, frame, gray, detections, pixel_cut=[3,4], size_I = [48,64]):\n",
    "        global fig_eye_pos\n",
    "        global fig_alpha\n",
    "        global fig_R_w\n",
    "        alpha_w2c = [0,0]\n",
    "        x_ratio = size_video[0]/self.size_df[0]\n",
    "        y_ratio = size_video[1]/self.size_df[1]\n",
    "        LE_M_A=[]\n",
    "        RE_M_A=[]\n",
    "        for k,bx in enumerate(detections):\n",
    "            # Get facial landmarks\n",
    "            time_start = time.time()\n",
    "            target_bx = dlib.rectangle(left=int(bx.left()*x_ratio),right =int(bx.right()*x_ratio),\n",
    "                                       top =int(bx.top()*y_ratio), bottom=int(bx.bottom()*y_ratio))\n",
    "            shape = self.predictor(gray, target_bx)\n",
    "            # get eye\n",
    "            LE_img, LE_M_A, LE_center, size_le_ori, R_le_LT = self.get_inputs(frame, shape, pos=\"L\", size_I=size_I)\n",
    "            RE_img, RE_M_A, RE_center, size_re_ori, R_re_LT = self.get_inputs(frame, shape, pos=\"R\", size_I=size_I)\n",
    "\n",
    "            alpha_w2c = self.shifting_angles_estimator(LE_center,RE_center)\n",
    "            \n",
    "            time_get_eye = time.time() - time_start\n",
    "            # gaze manipulation\n",
    "            time_start = time.time()\n",
    "            \n",
    "            # gaze redirection\n",
    "            # left Eye\n",
    "            LE_infer_img = L_sess.run(LE_img_pred, feed_dict= {\n",
    "                                                            LE_input_img: np.expand_dims(LE_img, axis = 0),\n",
    "                                                            LE_input_fp: np.expand_dims(LE_M_A, axis = 0),\n",
    "                                                            LE_input_ang: np.expand_dims(alpha_w2c, axis = 0),\n",
    "                                                            LE_phase_train: False\n",
    "                                                         })\n",
    "            LE_infer = cv2.resize(LE_infer_img.reshape(size_I[0],size_I[1],3), (size_le_ori[1], size_le_ori[0]))\n",
    "            # right Eye\n",
    "            RE_infer_img = R_sess.run(RE_img_pred, feed_dict= {\n",
    "                                                            RE_input_img: np.expand_dims(RE_img, axis = 0),\n",
    "                                                            RE_input_fp: np.expand_dims(RE_M_A, axis = 0),\n",
    "                                                            RE_input_ang: np.expand_dims(alpha_w2c, axis = 0),\n",
    "                                                            RE_phase_train: False\n",
    "                                                         })\n",
    "            RE_infer = cv2.resize(RE_infer_img.reshape(size_I[0],size_I[1],3), (size_re_ori[1], size_re_ori[0]))\n",
    "            \n",
    "            # repace eyes\n",
    "            frame[(R_le_LT[0]+pixel_cut[0]):(R_le_LT[0]+size_le_ori[0]-pixel_cut[0]),\n",
    "                  (R_le_LT[1]+pixel_cut[1]):(R_le_LT[1]+size_le_ori[1]-pixel_cut[1])] = LE_infer[pixel_cut[0]:(-1*pixel_cut[0]), pixel_cut[1]:-1*(pixel_cut[1])]*255\n",
    "            frame[(R_re_LT[0]+pixel_cut[0]):(R_re_LT[0]+size_re_ori[0]-pixel_cut[0]),\n",
    "                  (R_re_LT[1]+pixel_cut[1]):(R_re_LT[1]+size_re_ori[1]-pixel_cut[1])] = RE_infer[pixel_cut[0]:(-1*pixel_cut[0]), pixel_cut[1]:-1*(pixel_cut[1])]*255\n",
    "               \n",
    "        frame = self.monitor_para(frame)\n",
    "\n",
    "#         result, imgencode = cv2.imencode('.jpg', frame, encode_param)\n",
    "#         data = pickle.dumps(imgencode, 0)\n",
    "#         size = len(data)\n",
    "# #         print(\"{}: {}\".format(img_counter, size))\n",
    "#         client_socket.sendall(struct.pack(\">L\", size) + data)\n",
    "        cv2.imshow(\"Remote\",frame)    \n",
    "        return True\n",
    "        \n",
    " \n",
    "    def redirect_gaze(self, frame):\n",
    "        # head detection\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        face_detect_gray = cv2.resize(gray,(self.size_df[0],self.size_df[1]))\n",
    "        detections = self.detector(face_detect_gray, 0)\n",
    "           \n",
    "        rg_thread = Thread(target=self.flx_gaze, args=(frame, gray, detections))\n",
    "        rg_thread.start()\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def main():\n",
    "redir = False\n",
    "\n",
    "vs = cv2.VideoCapture(0)\n",
    "vs.set(3, size_video[0])\n",
    "vs.set(4, size_video[1])\n",
    "grs = gaze_redirection_system()\n",
    "t = time.time()\n",
    "cv2.namedWindow(conf.uid)\n",
    "cv2.moveWindow(conf.uid, int(Rs[0]/2)-int(size_window[0]/2),int(Rs[1]/2)-int(size_window[1]/2));\n",
    "global fps\n",
    "while 1:\n",
    "    ret, recv_frame = vs.read()\n",
    "    if ret:\n",
    "        cv2.imshow(conf.uid,recv_frame)\n",
    "        if recv_frame is not None:\n",
    "            # redirected gaze\n",
    "            if redir:\n",
    "                frame = recv_frame.copy()\n",
    "                try:\n",
    "                    tag = grs.redirect_gaze(frame)\n",
    "                except:\n",
    "                    pass\n",
    "            else:\n",
    "#                 result, imgencode = cv2.imencode('.jpg', recv_frame, encode_param)\n",
    "#                 data = pickle.dumps(imgencode, 0)\n",
    "#                 size = len(data)\n",
    "#                 client_socket.sendall(struct.pack(\">L\", size) + data)\n",
    "                cv2.imshow(\"Remote\",recv_frame)\n",
    "\n",
    "            if (time.time() - t) > 1:\n",
    "                t = time.time()\n",
    "                if conf.record_time:\n",
    "                    print('FPS %d' % fps)\n",
    "                fps = 0\n",
    "\n",
    "            k = cv2.waitKey(10)\n",
    "            if k == ord('q'):\n",
    "                break\n",
    "            elif k == ord('r'):\n",
    "                if redir:\n",
    "                    redir = False\n",
    "                else:\n",
    "                    redir = True\n",
    "            else:\n",
    "                pass\n",
    "# free\n",
    "# vr.stop()\n",
    "# client_socket.shutdown(socket.SHUT_RDWR)\n",
    "# client_socket.close()\n",
    "vs.release()\n",
    "cv2.destroyAllWindows()\n",
    "#     return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# client_socket.shutdown(socket.SHUT_RDWR)\n",
    "# client_socket.close()\n",
    "vs.release()\n",
    "cv2.destroyAllWindows()\n",
    "# if __name__ == '__main__':\n",
    "#     main()\n",
    "L_sess.close()\n",
    "R_sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
